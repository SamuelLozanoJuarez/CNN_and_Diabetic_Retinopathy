{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86469c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samue\\anaconda3\\envs\\venv_py39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#########################################################################################################################\n",
    "INFORMACIÓN DEL FICHERO\n",
    "#########################################################################################################################\n",
    "\n",
    "Autor: Samuel Lozano Juárez\n",
    "Fecha: 20/04/2023\n",
    "Institución: UBU | Grado en Ingeniería de la Salud\n",
    "\n",
    "Este archivo forma parte del Trabajo de Fin de Grado \"Detección del grado de retinopatía mediante redes convolucionales\".\n",
    "El alumno a cargo de este proyecto es el declarado como autor en las líneas anteriores.\n",
    "Los tutores del proyecto fueron el Dr. Darío Fernández Zoppino y el Dr. Daniel Urda Muñoz.\n",
    "\n",
    "En el código que se encuentra a continuación voy a crear una primera arquitectura de red neuronal convolucional, así como las demás \n",
    "estructuras necesarias para poder entrenar dicha red (como por ejemplo las funciones que permiten cargar las imágenes o generar los \n",
    "batches o lotes para el entrenamiento).\n",
    "\n",
    "El entrenamiento de la red se llevará a cabo empleando un conjunto de validación para poder aplicar la estrategia de Early Stopping y evitar el sobreentrenamiento.\n",
    "\n",
    "La arquitectura del modelo será básica, siguiendo la estructura que se encuentra disponible en la propia página del framework Pytorch:\n",
    "https://pytorch.org/tutorials/beginner/introyt/introyt1_tutorial.html#pytorch-models\n",
    "\n",
    "Finalmente se obtendrán las métricas de Accuracy, Balanced Accuracy, F-Score, AUC de la curva ROC y Quadratic Weighted Kappa, tanto para\n",
    "imágenes de iPhone como imágenes de Samsung. De esta manera podremos evaluar el rendimiento de la red en comparación con un clínico.\n",
    "'''\n",
    "\n",
    "#primero importamos todos los paquetes necesarios\n",
    "import torch #contiene todas las funciones de PyTorch\n",
    "import torch.nn as nn #contiene la clase padre de todos los modelos (nn.Module)\n",
    "import torch.nn.functional as F #esencial para la función de activación \n",
    "import torchvision #fundamental para la importación de imágenes\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt #para poder representar las gráficas\n",
    "import numpy as np #para las métricas de la red\n",
    "\n",
    "#importamos también las funciones definidas para el entrenamiento y puesta a prueba de los modelos\n",
    "from modules.CNN_utilities import entrena_val, representa_test, obtiene_metricas, tester, guarda_graficas\n",
    "\n",
    "#importamos el paquete que permite calcular el tiempo de entrenamiento\n",
    "import time\n",
    "\n",
    "#establecemos el tamaño del batch, la escala de las imágenes y el número de épocas de entrenamiento\n",
    "batch = 128 #es necesario aumentar el tamaño del Batch, para reducir los tiempos de entrenamiento\n",
    "escala = 640\n",
    "epocas = 150 #ya que tenemos activado Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bd4c7fe",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Found no valid file for the classes G1, G2, G3, G4, G5. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose(\n\u001b[0;32m      3\u001b[0m     [transforms\u001b[38;5;241m.\u001b[39mToTensor(), \u001b[38;5;66;03m#transforma la imagen de formato PIL a formato tensor\u001b[39;00m\n\u001b[0;32m      4\u001b[0m      transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m), (\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m)), \u001b[38;5;66;03m#normaliza el tensor para que la media de sus valores sea 0 y su desviación estándar 0.5\u001b[39;00m\n\u001b[0;32m      5\u001b[0m      transforms\u001b[38;5;241m.\u001b[39mResize((escala, escala))]) \u001b[38;5;66;03m#redimensionamos las imágenes\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#a continuación cargamos el conjunto de imágenes de train (Datasets) y los dos de test (iPhone y Samsung)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m Datasets \u001b[38;5;241m=\u001b[39m \u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDatos/Classified Data/Images/Datasets\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTamaño del conjunto de datos de train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(Datasets)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m Samsung \u001b[38;5;241m=\u001b[39m ImageFolder(root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDatos/Classified Data/Images/Samsung\u001b[39m\u001b[38;5;124m'\u001b[39m, transform \u001b[38;5;241m=\u001b[39m transform)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_py39\\lib\\site-packages\\torchvision\\datasets\\folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    303\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    308\u001b[0m ):\n\u001b[1;32m--> 309\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_py39\\lib\\site-packages\\torchvision\\datasets\\folder.py:145\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m    144\u001b[0m classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind_classes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot)\n\u001b[1;32m--> 145\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_to_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextensions \u001b[38;5;241m=\u001b[39m extensions\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_py39\\lib\\site-packages\\torchvision\\datasets\\folder.py:189\u001b[0m, in \u001b[0;36mDatasetFolder.make_dataset\u001b[1;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_to_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;66;03m# prevent potential bug since make_dataset() would use the class_to_idx logic of the\u001b[39;00m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;66;03m# find_classes() function, instead of using that of the find_classes() method, which\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;66;03m# is potentially overridden and thus could have a different logic.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe class_to_idx parameter cannot be None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmake_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_to_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_py39\\lib\\site-packages\\torchvision\\datasets\\folder.py:102\u001b[0m, in \u001b[0;36mmake_dataset\u001b[1;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    101\u001b[0m         msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupported extensions are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextensions \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(extensions, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(extensions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(msg)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m instances\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Found no valid file for the classes G1, G2, G3, G4, G5. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp"
     ]
    }
   ],
   "source": [
    "#a continuación definimos la operación que permitirá transformar las imágenes del repositorio en Tensores que puedan ser empleados por PyTorch\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), #transforma la imagen de formato PIL a formato tensor\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), #normaliza el tensor para que la media de sus valores sea 0 y su desviación estándar 0.5\n",
    "     transforms.Resize((escala, escala))]) #redimensionamos las imágenes\n",
    "\n",
    "#a continuación cargamos el conjunto de imágenes de train (Datasets) y los dos de test (iPhone y Samsung)\n",
    "Datasets = ImageFolder(root = 'Datos/Classified Data/Images/Datasets', transform = transform)\n",
    "print(f'Tamaño del conjunto de datos de train: {len(Datasets)}')\n",
    "\n",
    "Samsung = ImageFolder(root = 'Datos/Classified Data/Images/Samsung', transform = transform)\n",
    "print(f'Tamaño del conjunto de datos de test de Samsung: {len(Samsung)}')\n",
    "\n",
    "iPhone = ImageFolder(root = 'Datos/Classified Data/Images/iPhone', transform = transform)\n",
    "print(f'Tamaño del conjunto de datos de test de iPhone: {len(iPhone)}')\n",
    "\n",
    "#establecemos una lista con el nombre de las etiquetas\n",
    "classes = Datasets.classes\n",
    "\n",
    "#en esta ocasión, debido a que vamos a implementar EarlyStopping es necesario dividir el conjunto de entrenamiento en train y validation\n",
    "#Dividimos el conjunto de datos en entrenamiento y validación (80% y 20% respectivamente)\n",
    "train_size = int(0.8 * len(Datasets))\n",
    "val_size = len(Datasets) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(Datasets, [train_size, val_size])\n",
    "\n",
    "# Crear cargadores de datos para cada conjunto\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = batch, \n",
    "    shuffle = True,\n",
    "    num_workers = 8 #genera subprocesos para cargar los datos y así liberamos el proceso main\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset = val_dataset,\n",
    "    batch_size = batch,\n",
    "    shuffle = True,\n",
    "    num_workers = 8\n",
    ")\n",
    "\n",
    "test_S_loader = DataLoader(\n",
    "    dataset = Samsung,\n",
    "    batch_size = batch, #establecemos un tamaño de lote (batch_size) de 4, ya que son pocas imágenes y podemos permitírnoslo\n",
    "    shuffle = True, #indicamos que mezcle las imágenes\n",
    "    num_workers = 8 #genera subprocesos para cargar los datos y así liberamos el proceso main\n",
    ")\n",
    "\n",
    "test_i_loader = DataLoader(\n",
    "    dataset = iPhone,\n",
    "    batch_size = batch, #establecemos un tamaño de lote (batch_size) de 4, ya que son pocas imágenes y podemos permitírnoslo\n",
    "    shuffle = True, #indicamos que mezcle las imágenes\n",
    "    num_workers = 8 #genera subprocesos para cargar los datos y así liberamos el proceso main\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b52af98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#una vez que hemos comprobado que las funciones de carga funcionan correctamente ya podemos definir el modelo\n",
    "#los modelos se definen como clases que heredan todos ellos de un mismo padre: nn.Module\n",
    "#las clases contienen 2 funciones básicas: __init__() y forward()\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        #esta función sobreescribe la función init() del padre\n",
    "        super(CNN,self).__init__()\n",
    "        #definimos todas las capas que van a constituir el modelo\n",
    "        #una primera capa convolucional\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels = 3, #3 canales de entrada porque las imágenes son a color\n",
    "            out_channels = 6, #se trata del número de salidas de la capa. Es el número de kernels de la capa convolucional\n",
    "            kernel_size = 5, #suele tratarse de un número impar\n",
    "            stride = 1, #cantidad píxeles que se desplaza el filtro sobre la imagen\n",
    "            padding = 0, #cantidad de relleno que se va a aplicar sobre los bordes de la imagen\n",
    "        )\n",
    "        #una segunda capa convolucional\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels = 6, #6 canales de entrada porque es el número de salidas de la capa anterior\n",
    "            out_channels = 16, #se trata del número de salidas de la capa. Es el número de kernels de la capa convolucional\n",
    "            kernel_size = 5, #suele tratarse de un número impar\n",
    "            stride = 1, #cantidad píxeles que se desplaza el filtro sobre la imagen\n",
    "            padding = 0, #cantidad de relleno que se va a aplicar sobre los bordes de la imagen\n",
    "        )\n",
    "        \n",
    "        #una primera capa fully-connected (red neuronal propiamente dicha)\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features = 16*157*157, #número de parámetros de entrada de la red (los valores se obtienen experimentalmente)\n",
    "            out_features = 120 #número de neuronas de salida\n",
    "        )\n",
    "        \n",
    "        #una segunda fully-connected\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        \n",
    "        #y una tercera. Nótese que el número de neuronas de salida de la última fully-connected ha de coincidir con el número de clases\n",
    "        self.fc3 = nn.Linear(84,5)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #en esta función es donde tiene lugar la computación (y la función invocada por defecto al ejecutar la red)\n",
    "        #primero aplicamos la función ReLU a la capa convolucional, que simplifica los datos. \n",
    "        #ReLU Interpreta los valores positivos como son, y los negativos los torna 0, permitiendo acelerar el entrenamiento\n",
    "        #al resultado le aplicamos MaxPooling que reduce las dimensiones de los datos, seleccionando el valor máximo del kernel.\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), kernel_size = 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), kernel_size = 2)\n",
    "        #aplanamos la salida, hasta convertirla de forma matricial a forma vectorial (sería la capa flatten)\n",
    "        x = x.view(-1,self.num_flat_features(x))#usamos una función propia de la clase para obtener el número de características\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) #no incluimos una capa de LogSoft, que convierte el output en probabilidad, ya que la función loss que usaremos incluye esta funcionalidad\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self,x):\n",
    "        #por último definimos la función que permite obtener el número de características de los tensores\n",
    "        size = x.size()[1:] #seleccionamos todas las dimensiones expcepto la primera (que son los batches)\n",
    "        num_features = 1\n",
    "        #va iterando y calcula el número de características de los datos (x)\n",
    "        for s in size:\n",
    "            num_features*=s\n",
    "        return num_features\n",
    "\n",
    "#una vez definida la clase generamos una instancia de la misma\n",
    "cnn = CNN()\n",
    "\n",
    "#a continuación debemos entrenar el modelo, para ello es necesario definir una función loss que evalúa la desviación entre las predicciones y los valores reales\n",
    "#definimos como loss la función de tipo cross entropy \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "#y una función optimizadora que modificará los pesos de la red para tratar de mejorar su rendimiento\n",
    "#en este caso el optimizador será la función Adam (ampliamente utilizada)\n",
    "optimizer = torch.optim.Adam(params = cnn.parameters(), #los parámetros son los pesos que deberá ir actualizando el optimizador\n",
    "                             lr = 0.001) #dejamos el valor de learning rate por defecto (0.001)\n",
    "\n",
    "#previo al entrenamiento imprimimos por pantalla las características de la red, para poder identificar su entrenamiento\n",
    "print('--------------------------------------')\n",
    "print(f'Entrenamiento. Características:\\n  -Capas: 2\\n  -Neuronas: 120/84\\n  -Early Stopping\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b8a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#capturamos el tiempo antes del entrenamiento\n",
    "inicio = time.time()\n",
    "#entrenamos el modelo con 7 épocas de paciencia y recogemos las métricas para posteriormente guardar las gráficas\n",
    "acc,loss,val_acc,val_loss = entrena_val(cnn,epocas,7,train_loader,val_loader,optimizer,criterion)\n",
    "#y nuevamente calculamos el tiempo tras el entrenamiento\n",
    "fin = time.time()\n",
    "#posteriormente generamos y almacenamos las gráficas correspondientes a estas métricas\n",
    "guarda_graficas('Datasets','Si','No','No','RGB','Basica',2,1.0,'120/84',acc,loss,val_acc,val_loss)\n",
    "\n",
    "#ponemos a prueba la red con el conjunto de iPhone usando la función tester y recogemos los resultados para obtener las métricas\n",
    "y_true_iphone, y_pred_iphone, predictions_iphone = tester(cnn,test_i_loader)\n",
    "#obtenemos las métricas usando la función importada obtiene_metricas, que no las muestra por pantalla\n",
    "metricas_iphone = obtiene_metricas(y_true_iphone, y_pred_iphone, predictions_iphone)\n",
    "\n",
    "#las mostramos por pantalla\n",
    "print('\\n--------------------------------------')\n",
    "print(f'Test. Características:\\n  -Capas: 2\\n  -Neuronas: 120/84\\n  -Test:iphone\\n')\n",
    "print(f' - Matriz de confusión:\\n{metricas_iphone[0]}\\n - Accuracy:{metricas_iphone[1]}\\n - Balanced accuracy:{metricas_iphone[2]}\\n - F-score:{metricas_iphone[3]}\\n - Kappa:{metricas_iphone[4]}\\n - AUC:{metricas_iphone[5]}\\n - Tiempo:{(fin-inicio)/60} mins')\n",
    "#escribimos las métricas (a excepción de la matriz de confusión) en el archivo Resultados.csv previamente creado\n",
    "with open('Resultados.csv','a') as fd:\n",
    "    fd.write('\\n')\n",
    "    fd.write(f'Datasets,Sí,No,No,RGB,Básica,2,1.0,120/84,iphone,{metricas_iphone[1]},{metricas_iphone[2]},{metricas_iphone[3]},{metricas_iphone[4]},{metricas_iphone[5]},{(fin-inicio)/60}')\n",
    "\n",
    "\n",
    "#ahora ponemos a prueba la red con el conjunto de Samsung usando la función tester y recogemos los resultados para obtener las métricas\n",
    "y_true_samsung, y_pred_samsung, predictions_samsung = tester(cnn,test_S_loader)\n",
    "#obtenemos las métricas usando la función importada obtiene_metricas, que no las muestra por pantalla\n",
    "metricas_samsung = obtiene_metricas(y_true_samsung, y_pred_samsung, predictions_samsung)\n",
    "#las mostramos por pantalla\n",
    "print('\\n--------------------------------------')\n",
    "print(f'Test. Características:\\n  -Capas: 2\\n  -Neuronas: 120/84\\n  -Test:Samsung\\n')\n",
    "print(f' - Matriz de confusión:\\n{metricas_samsung[0]}\\n - Accuracy:{metricas_samsung[1]}\\n - Balanced accuracy:{metricas_samsung[2]}\\n - F-score:{metricas_samsung[3]}\\n - Kappa:{metricas_samsung[4]}\\n - AUC:{metricas_samsung[5]}\\n - Tiempo:{(fin-inicio)/60} mins')\n",
    "#escribimos las métricas (a excepción de la matriz de confusión) en el archivo Resultados.csv previamente creado\n",
    "with open('Resultados.csv','a') as fd:\n",
    "    fd.write('\\n')\n",
    "    fd.write(f'Datasets,Sí,No,No,RGB,Básica,2,1.0,120/84,Samsung,{metricas_samsung[1]},{metricas_samsung[2]},{metricas_samsung[3]},{metricas_samsung[4]},{metricas_samsung[5]},{(fin-inicio)/60}')\n",
    "    \n",
    "#por último vamos a guardar el modelo, sus pesos y estado actual, por si se quisiera volver a emplear\n",
    "torch.save(cnn.state_dict(), f'modelos/Basica/Datasets_Sival_Noprep_Noinp_RGB.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
