{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce415e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samue\\anaconda3\\envs\\venv_py39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#importamos los paquetes necesarios\n",
    "import torch #contiene todas las funciones de PyTorch\n",
    "import torch.nn as nn #contiene la clase padre de todos los modelos (nn.Module)\n",
    "import torch.nn.functional as F #esencial para la función de activación \n",
    "import torchvision #fundamental para la importación de imágenes\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "#importamos también matplotlib.pyplot y numpy para la visualización de las imágenes\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#importamos los paquetes necesarios para el cálculo de las métricas\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "#e importamos el paquete para el cálculo del tiempo de ejecución\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f98c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#establecemos el tamaño del batch, la escala de las imágenes y el número de épocas de entrenamiento\n",
    "batch = 4\n",
    "escala = 640\n",
    "epocas = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76c4455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a continuación definimos la operación que permitirá transformar las imágenes del repositorio en Tensores que puedan ser empleados por PyTorch\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), #transforma la imagen de formato PIL a formato tensor\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), #normaliza el tensor para que la media de sus valores sea 0 y su desviación estándar 0.5\n",
    "     transforms.Resize((escala, escala))]) #redimensionamos las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38427487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de datos de train: 113\n",
      "Tamaño del conjunto de datos de test de Samsung: 93\n",
      "Tamaño del conjunto de datos de test de iPhone: 99\n"
     ]
    }
   ],
   "source": [
    "#a continuación cargamos el conjunto de imágenes de train (OCT) y los dos de test (iPhone y Samsung)\n",
    "OCT = ImageFolder(root = 'Datos/Classified Data/Images/OCT', transform = transform)\n",
    "print(f'Tamaño del conjunto de datos de train: {len(OCT)}')\n",
    "\n",
    "Samsung = ImageFolder(root = 'Datos/Classified Data/Images/Samsung', transform = transform)\n",
    "print(f'Tamaño del conjunto de datos de test de Samsung: {len(Samsung)}')\n",
    "\n",
    "iPhone = ImageFolder(root = 'Datos/Classified Data/Images/iPhone', transform = transform)\n",
    "print(f'Tamaño del conjunto de datos de test de iPhone: {len(iPhone)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d9112ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#establecemos una lista con el nombre de las etiquetas\n",
    "classes = OCT.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5c6036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#en esta ocasión, debido a que vamos a implementar EarlyStopping es necesario dividir el conjunto de entrenamiento en train y validation\n",
    "#Dividimos el conjunto de datos en entrenamiento y validación\n",
    "train_size = int(0.8 * len(OCT))\n",
    "val_size = len(OCT) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(OCT, [train_size, val_size])\n",
    "\n",
    "# Crear cargadores de datos para cada conjunto\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = 4, \n",
    "    shuffle = True,\n",
    "    num_workers = 2 #genera subprocesos para cargar los datos y así liberamos el proceso main\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset = val_dataset,\n",
    "    batch_size = 4,\n",
    "    shuffle = True,\n",
    "    num_workers = 2\n",
    ")\n",
    "\n",
    "test_S_loader = DataLoader(\n",
    "    dataset = Samsung,\n",
    "    batch_size = 4, #establecemos un tamaño de lote (batch_size) de 10, ya que son pocas imágenes y podemos permitírnoslo\n",
    "    shuffle = True, #indicamos que mezcle las imágenes\n",
    "    num_workers = 2 #genera subprocesos para cargar los datos y así liberamos el proceso main\n",
    ")\n",
    "\n",
    "test_i_loader = DataLoader(\n",
    "    dataset = iPhone,\n",
    "    batch_size = 4, #establecemos un tamaño de lote (batch_size) de 10, ya que son pocas imágenes y podemos permitírnoslo\n",
    "    shuffle = True, #indicamos que mezcle las imágenes\n",
    "    num_workers = 2 #genera subprocesos para cargar los datos y así liberamos el proceso main\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6f6f457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=394384, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#a continuación debemos definir el modelo\n",
    "#una vez que hemos comprobado que las funciones de carga funcionan correctamente ya podemos definir el modelo\n",
    "#los modelos se definen como clases que heredan todos ellos de un mismo padre: nn.Module\n",
    "#las clases contienen 2 funciones básicas: __init__() y forward()\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        #esta función sobreescribe la función init() del padre\n",
    "        super(CNN,self).__init__()\n",
    "        #definimos todas las capas que van a constituir el modelo\n",
    "        #una primera capa convolucional\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels = 3, #3 canales de entrada porque las imágenes son a color\n",
    "            out_channels = 6, #se trata del número de salidas de la capa. Es el número de kernels de la capa convolucional\n",
    "            kernel_size = 5, #suele tratarse de un número impar\n",
    "            stride = 1, #cantidad píxeles que se desplaza el filtro sobre la imagen\n",
    "            padding = 0, #cantidad de relleno que se va a aplicar sobre los bordes de la imagen\n",
    "        )\n",
    "        #una segunda capa convolucional\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels = 6, #6 canales de entrada porque es el número de salidas de la capa anterior\n",
    "            out_channels = 16, #se trata del número de salidas de la capa. Es el número de kernels de la capa convolucional\n",
    "            kernel_size = 5, #suele tratarse de un número impar\n",
    "            stride = 1, #cantidad píxeles que se desplaza el filtro sobre la imagen\n",
    "            padding = 0, #cantidad de relleno que se va a aplicar sobre los bordes de la imagen\n",
    "        )\n",
    "        \n",
    "        #una primera capa fully-connected (red neuronal propiamente dicha)\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features = 16*157*157, #número de parámetros de entrada de la red (los valores se obtienen experimentalmente)\n",
    "            out_features = 120 #número de neuronas de salida\n",
    "        )\n",
    "        \n",
    "        #una segunda fully-connected\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        \n",
    "        #y una tercera. Nótese que el número de neuronas de salida de la última fully-connected ha de coincidir con el número de clases\n",
    "        self.fc3 = nn.Linear(84,5)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #en esta función es donde tiene lugar la computación (y la función invocada por defecto al ejecutar la red)\n",
    "        #primero aplicamos la función ReLU a la capa convolucional, que simplifica los datos. \n",
    "        #ReLU Interpreta los valores positivos como son, y los negativos los torna 0, permitiendo acelerar el entrenamiento\n",
    "        #al resultado le aplicamos MaxPooling que reduce las dimensiones de los datos, seleccionando el valor máximo del kernel.\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), kernel_size = 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), kernel_size = 2)\n",
    "        #aplanamos la salida, hasta convertirla de forma matricial a forma vectorial (sería la capa flatten)\n",
    "        x = x.view(-1,self.num_flat_features(x))#usamos una función propia de la clase para obtener el número de características\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) #no incluimos una capa de LogSoft, que convierte el output en probabilidad, ya que la función loss que usaremos incluye esta funcionalidad\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self,x):\n",
    "        #por último definimos la función que permite obtener el número de características de los tensores\n",
    "        size = x.size()[1:] #seleccionamos todas las dimensiones expcepto la primera (que son los batches)\n",
    "        num_features = 1\n",
    "        #va iterando y calcula el número de características de los datos (x)\n",
    "        for s in size:\n",
    "            num_features*=s\n",
    "        return num_features\n",
    "\n",
    "#una vez definida la clase generamos una instancia de la misma\n",
    "cnn = CNN()\n",
    "#y mostramos por pantalla sus secciones\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e177aa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a continuación debemos entrenar el modelo, para ello es necesario definir una función loss que evalúa la desviación entre las predicciones y los valores reales\n",
    "#definimos como loss la función de tipo cross entropy \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "#y una función optimizadora que modificará los pesos de la red para tratar de mejorar su rendimiento\n",
    "#en este caso el optimizador será la función Adam (ampliamente utilizada)\n",
    "optimizer = torch.optim.Adam(params = cnn.parameters(), #los parámetros son los pesos que deberá ir actualizando el optimizador\n",
    "                             lr = 0.001) #dejamos el valor de learning rate por defecto (0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc385ab",
   "metadata": {},
   "source": [
    "HAY UNA MANERA DE HACER EARLYSTOPPING EMPLEANDO LA LIBRERÍA PYTORCHTOOLS, QUE INCLUYE UNA PROPIA FUNCIÓN DE EARLYSTOPPING, PERO PARA SU USO ES NECESARIO QUE LAS DEPENDENCIAS SE ENCUENTREN EN EL MISMO DIRECTORIO QUE EL SCRIPT EN CUESTIÓN, Y ESO EN SCAYLE GENERA PROBLEMAS, es más jaleo. Por eso prefiero hacerlo de una manera más manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c08060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vamos a definir la función para el entrenamiento\n",
    "def entrena_val(red,epocas,paciencia,train_loader,val_loader,optimizer,criterion):\n",
    "    #primero importamos los paquetes necesarios\n",
    "    import torch\n",
    "    #inicializamos best_val_loss (que es el parámetro que va a marcar el Early Stopping) como infinito\n",
    "    best_val_loss = float('inf')\n",
    "    #creamos también una variable para almacenar los parámetros del mejor modelo (aquel con menor val_loss)\n",
    "    best_model_params = None\n",
    "    #iniciamos también un contador, para poder aplicar Early Stopping con la paciencia deseada\n",
    "    contador = 0\n",
    "    #definimos 2 listas en las que almacenaremos los valores de accuracy y loss de train cada época para devolverlas\n",
    "    acc_graph = []\n",
    "    loss_graph = []\n",
    "    #y 2 listas en las que almacenaremos los valores de accuracy y loss de validación cada época para poder devolverlas\n",
    "    val_acc_graph = []\n",
    "    val_loss_graph = []\n",
    "    \n",
    "    #para entrenar el modelo vamos a iterar el número de épocas determinadas, calculando el valor de loss y accuracy para cada época\n",
    "    for epoch in range(epocas):\n",
    "        #establecemos el número de predicciones correctas inicial a 0\n",
    "        correct = 0\n",
    "        #y cargamos las imágenes de entrenamiento y sus etiquetas usando la estructura Loader previamente creada\n",
    "        for data in train_loader:\n",
    "            inputs, labels = data\n",
    "            #establecemos a 0 los parámetros del modelo\n",
    "            optimizer.zero_grad()\n",
    "            #generamos las predicciones de los inputs\n",
    "            outputs = red(inputs)\n",
    "            #calculamos el loss, la desviación de las predicciones con respecto a las etiquetas\n",
    "            loss = criterion(outputs, labels)\n",
    "            #propagamos hacia atrás el valor loss\n",
    "            loss.backward()\n",
    "            #y modificamos los pesos en función del loss y la función optimizer\n",
    "            optimizer.step()\n",
    "            #actualizamos el número de predicciones correctas\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "        #una vez finalizada la época (que recorre todo el conjunto de imágenes) mostramos el valor del loss y del accuracy\n",
    "        print(f'Época {epoch +1}/{epocas} - Accuracy: {correct/len(train_loader.dataset)} - Loss: {loss.data.item()}')\n",
    "        #añadimos los valores a la lista correspondiente\n",
    "        loss_graph.append(loss.data.item())\n",
    "        acc_graph.append(correct/len(train_loader.dataset))\n",
    "\n",
    "        #realizamos ahora las iteraciones correspondientes a las imágenes de validación\n",
    "        #primero establecemos el valor del loss de validación a cero\n",
    "        val_loss = 0.0\n",
    "        #establecemos así mismo el número de predicciones correctas nuevamente a cero\n",
    "        correct = 0\n",
    "        #cargamos las imágenes de validación y sus etiquetas\n",
    "        for data in val_loader:\n",
    "            inputs, labels = data\n",
    "            #generamos las predicciones a partir de los inputs\n",
    "            outputs = red(inputs)\n",
    "            #calculamos el loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            #y lo vamos acumulando\n",
    "            val_loss += loss.item()\n",
    "            #finalmente calculamos el número de predicciones correctas\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        #una vez finalizada la época (que recorre todo el conjunto de imágenes) mostramos el valor del loss y del accuracy de validación\n",
    "        print(f'Época {epoch +1}/{epocas} - Val_accuracy: {correct/len(val_loader.dataset)} - Val_loss: {loss.data.item()}')\n",
    "        #añadimos los valores a la lista correspondiente\n",
    "        val_loss_graph.append(loss.data.item())\n",
    "        val_acc_graph.append(correct/len(val_loader.dataset))\n",
    "        \n",
    "        #finalmente solo falta realizar la comprobación del Early Stopping\n",
    "        #si el valor de val_loss de esta época es inferior al mejor conseguido hasta el momento:\n",
    "        if val_loss < best_val_loss:\n",
    "            #entonces actualiza el valor del mejor val_loss (ya que lo que queremos es minimizar este valor)\n",
    "            best_val_loss = val_loss\n",
    "            #posteriormente guarda el estado del modelo actual\n",
    "            best_model_params = red.state_dict()\n",
    "            #y vuelve a establecer el contador de paciencia a 0\n",
    "            contador = 0\n",
    "        #si el valor de val_loss no disminuye (no mejora) con respecto al último mejor:\n",
    "        else:\n",
    "            #si se ha llegado al límite de la paciencia establecida detiene el entrenamiento para evitar el sobreentrenamiento\n",
    "            if contador == paciencia:\n",
    "                break\n",
    "            #si aún no ha llegado al límite de la paciencia entonces incrementa el contador en uno y sigue entrenando\n",
    "            else:\n",
    "                contador += 1\n",
    "        \n",
    "        #finalmente, una vez finalizado el entrenamiento se debe cargar el mejor estado del modelo\n",
    "        red.load_state_dict(best_model_params)\n",
    "        \n",
    "        #y devolver las métricas almacenadas de entrenamiento y validación\n",
    "        return acc_graph, loss_graph, val_acc_graph, val_loss_graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
