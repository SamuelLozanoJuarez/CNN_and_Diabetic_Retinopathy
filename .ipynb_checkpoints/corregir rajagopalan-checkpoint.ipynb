{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab50ad66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samue\\anaconda3\\envs\\venv_py39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#primero importamos todos los paquetes necesarios\n",
    "import torch #contiene todas las funciones de PyTorch\n",
    "import torch.nn as nn #contiene la clase padre de todos los modelos (nn.Module)\n",
    "import torch.nn.functional as F #esencial para la función de activación \n",
    "import torchvision #fundamental para la importación de imágenes\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt #para poder representar las gráficas\n",
    "import numpy as np #para las métricas de la red\n",
    "\n",
    "import time\n",
    "\n",
    "#importamos también las funcioness definidas para el entrenamiento y puesta a prueba de los modelos\n",
    "from modules.CNN_utilities import entrena, representa_test, obtiene_metricas, tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b95bd4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#establecemos el tamaño del batch, la escala de las imágenes y el número de épocas de entrenamiento\n",
    "batch = 4\n",
    "#la arquitectura propuesta por Rajagopalan requiere una escala de 224, 224, 3\n",
    "escala = 224\n",
    "epocas = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0fefadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de datos de train: 113\n",
      "Tamaño del conjunto de datos de test de Samsung: 93\n",
      "Tamaño del conjunto de datos de test de iPhone: 99\n"
     ]
    }
   ],
   "source": [
    "#volvemos a crear las funciones necesarias para la carga de las imágenes, debido al cambio en la escala\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), #transforma la imagen de formato PIL a formato tensor\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), #normaliza el tensor para que la media de sus valores sea 0 y su desviación estándar 0.5\n",
    "     transforms.Resize((escala, escala))]) #redimensionamos las imágenes\n",
    "\n",
    "#a continuación cargamos el conjunto de imágenes de train (OCT) y los dos de test (iPhone y Samsung)\n",
    "OCT = ImageFolder(root = 'Datos/Classified Data/Images/OCT', transform = transform)\n",
    "print(f'Tamaño del conjunto de datos de train: {len(OCT)}')\n",
    "\n",
    "Samsung = ImageFolder(root = 'Datos/Classified Data/Images/Samsung', transform = transform)\n",
    "print(f'Tamaño del conjunto de datos de test de Samsung: {len(Samsung)}')\n",
    "\n",
    "iPhone = ImageFolder(root = 'Datos/Classified Data/Images/iPhone', transform = transform)\n",
    "print(f'Tamaño del conjunto de datos de test de iPhone: {len(iPhone)}')\n",
    "\n",
    "#establecemos una lista con el nombre de las etiquetas\n",
    "classes = OCT.classes\n",
    "\n",
    "#y definimos también las funciones que van a ir cargando las imágenes en el modelo\n",
    "train_loader = DataLoader(\n",
    "    dataset = OCT,\n",
    "    batch_size = 4, #establecemos un tamaño de lote (batch_size) de 4, ya que son pocas imágenes y podemos permitírnoslo\n",
    "    shuffle = True, #indicamos que mezcle las imágenes\n",
    "    num_workers = 2 #genera subprocesos para cargar los datos y así liberamos el proceso main\n",
    ")\n",
    "\n",
    "test_S_loader = DataLoader(\n",
    "    dataset = Samsung,\n",
    "    batch_size = 4, #establecemos un tamaño de lote (batch_size) de 10, ya que son pocas imágenes y podemos permitírnoslo\n",
    "    shuffle = True, #indicamos que mezcle las imágenes\n",
    "    num_workers = 2 #genera subprocesos para cargar los datos y así liberamos el proceso main\n",
    ")\n",
    "\n",
    "test_i_loader = DataLoader(\n",
    "    dataset = iPhone,\n",
    "    batch_size = 4, #establecemos un tamaño de lote (batch_size) de 10, ya que son pocas imágenes y podemos permitírnoslo\n",
    "    shuffle = True, #indicamos que mezcle las imágenes\n",
    "    num_workers = 2 #genera subprocesos para cargar los datos y así liberamos el proceso main\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f5d36db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A lo largo de este script voy a probar a variar algunos parámetros del modelo (intentando no perder la esencia de la estructura original)\n",
    "#Los parámetros modificados serán los siguientes:\n",
    "# - número de capas convolucionales (3,5 o 7)\n",
    "# - número de filtros por capa (conservando los originales, reduciéndolos a la mitad o multiplicándolos por dos)\n",
    "# - número de neuronas de las capas fully-connected, probando las siguientes combinaciones:\n",
    "#    * 512/256\n",
    "#    * 1024/512\n",
    "#    * 256/128\n",
    "#    * 128/64\n",
    "#    * 64/128\n",
    "#    * 128/256\n",
    "# Por tanto el número total de posibles combinaciones es 3*3*6 = 54 combinaciones\n",
    "\n",
    "#Para facilitar la lectura del código y sobre todo su ejecución, al igual que en ocasiones anteriores voy a definir una función que permita lanzar las ejecuciones necesarias de manera automática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35b8d61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crea_Rajagopalan(capas_conv, filtros, neuronas):\n",
    "    '''\n",
    "    Función que crea una red siguiendo la arquitectura Ghosh pero con las características introducidas como parámetros.\n",
    "    \n",
    "    Parámetros\n",
    "    --------------------------------------------------------------------------\n",
    "    capas_conv: número entero que puede tomar 3 posibles valores (3,5 o 7) y que representa el número de capas convolucionales que tiene la red.\n",
    "    filtros: float que representa el número de filtros por capa convolucional. Puede ser 1.0 si conserva el número original, 0.5 si lo divide a la mitad y 2.0 si lo duplica.\n",
    "    neuronas: String que contiene el número de neuronas de las capas fully-connected separados por barras laterales (/).\n",
    "    \n",
    "    Return\n",
    "    --------------------------------------------------------------------------\n",
    "    modelo: devuelve una instancia de la clase Rajagopalan con las características arquitectónicas deseadas, es decir, un modelo de CNN con las características indicadas en los parámetros.\n",
    "    '''\n",
    "    \n",
    "    #Debido a las distintas variaciones que se van a producir en la arquitectura, el número de características (y por tanto de neuronas) variará.\n",
    "    #Es por ello que para simplificar esta labor voy a definir 2 funciones que permiten calcular el número de características resultantes tras las capas convolucionales y de MaxPooling.\n",
    "    #De esta manera se podrá pasar este valor como parámetro in_features a la primera capa fully-connected.\n",
    "\n",
    "    #Sabemos que el número de características tras una capa convolucional se corresponde con la siguiente fórmula:\n",
    "    #    features = num_filtros*ancho*alto\n",
    "    #Tal y como se describe en el capítulo \"Convolutional Neural Networks\" en el libro \"Deep Learning\" de Ian Goodfellow, Yoshua Bengio y Aaron Courville\n",
    "\n",
    "    #El número de filtros lo podemos obtener de la última capa convolucional, pero el ancho y alto de la imagen variarán tras cada capa.\n",
    "    #Las dimensiones modificadas tras una capa convolucional y de maxpooling se pueden calcular gracias a la siguiente ecuación:\n",
    "    #    output_size = (input_size-2*padding-kernel_size)/stride + 1\n",
    "    #Tal y como se describe en el capítulo 3 \"Convolutional Neural Networks\" del libro \"Deep Learning for Computer Vision\" de Rajalingappaa Shanmugamani\n",
    "\n",
    "    #Sabiendo esto ya podemos definir las funciones que permiten calcular las dimensiones\n",
    "    def funcion(input_size,kernel_size,stride,padding):\n",
    "        '''\n",
    "        Aplica la ecuación para calcular las dimensiones tras una capa convolucional/maxpooling descrita en el libro Deep Learning for Computer Vision.\n",
    "\n",
    "        Parámetros\n",
    "        ----------------------------------------------------\n",
    "        input_size: número entero que se corresponde con la escala de la imagen inicial, previa a la capa convolucional.\n",
    "        kernel_size: número entero que representa el tamaño del filtro de la capa.\n",
    "        stride:  número entero que representa el desplazamiento del filtro sobre la imagen.\n",
    "        padding: número entero correspondiente al número de píxeles de relleno.\n",
    "\n",
    "        Return\n",
    "        ----------------------------------------------------\n",
    "        Devuelve un número flotante correspondiente al tamaño de la imagen una vez modificada por la capa convolucional/maxpooling.\n",
    "        '''\n",
    "        #aplicamos la fórmula\n",
    "        return ((input_size + 2*padding - kernel_size)/stride + 1)\n",
    "\n",
    "    def calcula_dim(num_capas):\n",
    "        '''\n",
    "        Calcula la escala de una imagen tras haber sido transformada por n capas convolucionales y de maxpooling, según las características de la arquitectura Ghosh.\n",
    "\n",
    "        Parámetros\n",
    "        ----------------------------------------------------\n",
    "        num_capas:\n",
    "\n",
    "        Return\n",
    "        ----------------------------------------------------\n",
    "        Devuelve un número entero correspondiente a la escala de la imagen después de todas las capas convolucionales y de maxpooling.\n",
    "        '''\n",
    "        #el tamaño inicial de las imágenes es de 512x512 (la escala original)\n",
    "        size = 224\n",
    "\n",
    "        #vamos actualizando el tamaño de la imagen según vaya atravesando capas convolucionales y de maxpooling\n",
    "        size = funcion(size,9,4,0)#tras primera capa convolucional\n",
    "        size = funcion(size,2,2,0)#tras maxpooling\n",
    "        size = funcion(size,7,1,0)#tras segunda capa convolucional\n",
    "        size = funcion(size,2,2,0)#tras maxpooling\n",
    "        size = funcion(size,5,1,0)#tras tercera capa convolucional\n",
    "        \n",
    "        if num_capas >3:\n",
    "            size = funcion(size,3,1,0)#tras cuarta capa convolucional\n",
    "            size = funcion(size,3,1,0)#tras quinta capa convolucional\n",
    "            \n",
    "            if num_capas >5:\n",
    "                size = funcion(size,2,1,0)#tras sexta capa convolucional\n",
    "                size = funcion(size,1,1,0)#tras séptima capa convolucional\n",
    "        \n",
    "        return int(size)\n",
    "\n",
    "    #primero definimos la clase correspondiente (Rajagopalan en este caso), incluyendo los elementos necesarios para obtener las variaciones deseadas\n",
    "    class Rajagopalan(nn.Module):\n",
    "        #esta estructura está formada por capas convolucionales, de maxpooling, de activación, de Dropout, fully-connected y de clasificación\n",
    "\n",
    "        def __init__(self):\n",
    "            #sobreescribimos el constructor del padre\n",
    "            super(Rajagopalan,self).__init__()\n",
    "            #primero definimos una capa convolucional\n",
    "            #el número de filtros de cada capa irá multiplicado por el parámetro filtros (para reducirlo, duplicarlo o mantenerlo)\n",
    "            self.conv1 = nn.Conv2d(\n",
    "                in_channels = 3, #3 canales de entrada porque las imágenes son a color\n",
    "                out_channels = int(64*filtros), #se trata del número de salidas de la capa. Es el número de kernels de la capa convolucional\n",
    "                kernel_size = 9, #suele tratarse de un número impar\n",
    "                stride = 4, #cantidad píxeles que se desplaza el filtro sobre la imagen\n",
    "                padding = 0, #cantidad de relleno que se va a aplicar sobre los bordes de la imagen\n",
    "            )\n",
    "\n",
    "            #una segunda convolucional\n",
    "            self.conv2 = nn.Conv2d(\n",
    "                in_channels = int(64*filtros), #64 canales de entrada porque es el número de salidas de la capa anterior\n",
    "                out_channels = int(128*filtros), #se trata del número de salidas de la capa. Es el número de kernels de la capa convolucional\n",
    "                kernel_size = 7, #suele tratarse de un número impar\n",
    "                stride = 1, #cantidad píxeles que se desplaza el filtro sobre la imagen\n",
    "                padding = 0, #cantidad de relleno que se va a aplicar sobre los bordes de la imagen\n",
    "            )\n",
    "\n",
    "            #la tercera convolucional\n",
    "            self.conv3 = nn.Conv2d(\n",
    "                in_channels = int(128*filtros), #128 canales de entrada porque es el número de salidas de la capa anterior\n",
    "                out_channels = int(256*filtros), #se trata del número de salidas de la capa. Es el número de kernels de la capa convolucional\n",
    "                kernel_size = 5, #suele tratarse de un número impar\n",
    "                stride = 1, #cantidad píxeles que se desplaza el filtro sobre la imagen\n",
    "                padding = 0, #cantidad de relleno que se va a aplicar sobre los bordes de la imagen\n",
    "            )\n",
    "\n",
    "            if capas_conv >3:\n",
    "                #cuarta convolucional\n",
    "                self.conv4 = nn.Conv2d(\n",
    "                    in_channels = int(256*filtros), #256 canales de entrada porque es el número de salidas de la capa anterior\n",
    "                    out_channels = int(384*filtros), #se trata del número de salidas de la capa. Es el número de kernels de la capa convolucional\n",
    "                    kernel_size = 3, #suele tratarse de un número impar\n",
    "                    stride = 1, #cantidad píxeles que se desplaza el filtro sobre la imagen\n",
    "                    padding = 0, #cantidad de relleno que se va a aplicar sobre los bordes de la imagen\n",
    "                )\n",
    "\n",
    "                #quinta y última capa convolucional\n",
    "                self.conv5 = nn.Conv2d(\n",
    "                    in_channels = int(384*filtros), #256 canales de entrada porque es el número de salidas de la capa anterior\n",
    "                    out_channels = int(256*filtros), #se trata del número de salidas de la capa. Es el número de kernels de la capa convolucional\n",
    "                    kernel_size = 3, #suele tratarse de un número impar\n",
    "                    stride = 1, #cantidad píxeles que se desplaza el filtro sobre la imagen\n",
    "                    padding = 0, #cantidad de relleno que se va a aplicar sobre los bordes de la imagen\n",
    "                )\n",
    "\n",
    "                if capas_conv >5:\n",
    "                    self.conv6 = nn.Conv2d(\n",
    "                        in_channels = int(256*filtros), #256 canales de entrada porque es el número de salidas de la capa anterior\n",
    "                        out_channels = int(128*filtros), #se trata del número de salidas de la capa. Es el número de kernels de la capa convolucional\n",
    "                        kernel_size = 2, #suele tratarse de un número impar\n",
    "                        stride = 1, #cantidad píxeles que se desplaza el filtro sobre la imagen\n",
    "                        padding = 0, #cantidad de relleno que se va a aplicar sobre los bordes de la imagen\n",
    "                    )\n",
    "\n",
    "                    self.conv7 = nn.Conv2d(\n",
    "                        in_channels = int(128*filtros), #128 canales de entrada porque es el número de salidas de la capa anterior\n",
    "                        out_channels = int(128*filtros), #se trata del número de salidas de la capa. Es el número de kernels de la capa convolucional\n",
    "                        kernel_size = 1, #suele tratarse de un número impar\n",
    "                        stride = 1, #cantidad píxeles que se desplaza el filtro sobre la imagen\n",
    "                        padding = 0, #cantidad de relleno que se va a aplicar sobre los bordes de la imagen\n",
    "                    )\n",
    "                    \n",
    "            #usando la función descrita anteriormente y la ecuación features = num_filtros*ancho*alto podemos calcular el número de características\n",
    "            #si el número de capas convolucionales es 3, el número de filtros será 256 multiplicado por el parámetro filtros \n",
    "            if capas_conv == 3:\n",
    "                neuronas_entrada = int(256*filtros)*calcula_dim(capas_conv)*calcula_dim(capas_conv)\n",
    "            #si el número de capas convolucionales es 5, el número de filtros será 256 multiplicado por el parámetro filtros\n",
    "            elif capas_conv == 5:\n",
    "                neuronas_entrada = int(256*filtros)*calcula_dim(capas_conv)*calcula_dim(capas_conv)\n",
    "            #si el número de capas convolucionales es 7, el número de filtros será 128 multiplicado por el parámetro filtros\n",
    "            else:\n",
    "                neuronas_entrada = int(128*filtros)*calcula_dim(capas_conv)*calcula_dim(capas_conv)\n",
    "\n",
    "            #definimos también la función de MaxPooling que se aplicará sobre algunas de las capas convolucionales\n",
    "            self.pool = nn.MaxPool2d(\n",
    "                kernel_size = 2, #establecemos el tamaño del kernel a 2*2\n",
    "                stride = 2 #cantidad píxeles que se desplaza el filtro sobre la imagen (por defecto se desplazará el tamaño del kernel)\n",
    "            )\n",
    "\n",
    "            #y las capas de neuronas fully-connected\n",
    "            self.fc1 = nn.Linear(\n",
    "                in_features = neuronas_entrada, #número de parámetros de entrada de la red (los valores se obtienen experimentalmente)\n",
    "                out_features = int(neuronas.split('/')[0]) #número de neuronas de salida, obtenidas del parámetro pasado\n",
    "            )\n",
    "\n",
    "            self.fc2 = nn.Linear(int(neuronas.split('/')[0]),int(neuronas.split('/')[1]))\n",
    "\n",
    "            #y por último la capa encargada de realizar las predicciones\n",
    "            self.dense = nn.Linear(int(neuronas.split('/')[1]),5)#tiene 5 neuronas de salida, una para cada clase de nuestro problema\n",
    "            \n",
    "        def forward(self,x):\n",
    "            #en esta función es donde tiene lugar la computación (y la función invocada por defecto al ejecutar la red)\n",
    "            #siguiendo la estructura descrita en Ghosh et al. (con sus respectivas variaciones):\n",
    "            \n",
    "            #primero una capa convolucional con activación ReLU y maxpooling\n",
    "            x = self.pool(F.relu(self.conv1(x)))\n",
    "            #una segunda capa convolucional con activación y maxpooling\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "            #a continuación 3 capas convolucionales SIN maxpooling (pero sí activación ReLU)\n",
    "            x = F.relu(self.conv3(x))\n",
    "            if capas_conv >3:\n",
    "                x = F.relu(self.conv4(x))\n",
    "                x = F.relu(self.conv5(x))\n",
    "                if capas_conv >5:\n",
    "                    x = F.relu(self.conv6(x))\n",
    "                    x = F.relu(self.conv7(x))\n",
    "                \n",
    "            #aplanamos la salida usando la función view para convertir las dimensiones de los datos\n",
    "            x = x.view(-1,self.num_flat_features(x))#usamos una función propia de la clase para obtener el número de características\n",
    "            #posteriormente tienen lugar las dos capas fully-connected\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            #y por último la capa densa que va a proporcionarnos la predicción\n",
    "            x = self.dense(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "        def num_flat_features(self,x):\n",
    "            #por último definimos la función que permite obtener el número de características de los tensores\n",
    "            size = x.size()[1:] #seleccionamos todas las dimensiones expcepto la primera (que son los batches)\n",
    "            num_features = 1\n",
    "            #va iterando y calcula el número de características de los datos (x)\n",
    "            for s in size:\n",
    "                num_features*=s\n",
    "            return num_features\n",
    "\n",
    "    #por último creamos una instancia de esta red\n",
    "    modelo = Rajagopalan()\n",
    "    #y la devolvemos\n",
    "    return modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51002f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entra aquí\n",
      "entra aquí\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x00000193C272CAF0>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\samue\\anaconda3\\envs\\venv_py39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"C:\\Users\\samue\\anaconda3\\envs\\venv_py39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1430, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"C:\\Users\\samue\\anaconda3\\envs\\venv_py39\\lib\\multiprocessing\\process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"C:\\Users\\samue\\anaconda3\\envs\\venv_py39\\lib\\multiprocessing\\popen_spawn_win32.py\", line 108, in wait\n",
      "    res = _winapi.WaitForSingleObject(int(self._handle), msecs)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4x1024 and 512x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#en este caso el optimizador será la función Adam (ampliamente utilizada)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(params \u001b[38;5;241m=\u001b[39m modelo\u001b[38;5;241m.\u001b[39mparameters()) \u001b[38;5;66;03m#dejamos el valor de learning-rate por defecto (0.001)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m acc,loss \u001b[38;5;241m=\u001b[39m \u001b[43mentrena\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelo\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepocas\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\UBU\\4º curso\\TFG\\CNN_and_Diabetic_Retinopathy\\modules\\CNN_utilities.py:53\u001b[0m, in \u001b[0;36mentrena\u001b[1;34m(red, epocas, train_loader, optimizer, criterion)\u001b[0m\n\u001b[0;32m     51\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m#generamos las predicciones a partir de los inputs\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mred\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m#calculamos el loss, la desviación de las predicciones con respecto a las etiquetas\u001b[39;00m\n\u001b[0;32m     55\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36mcrea_Rajagopalan.<locals>.Rajagopalan.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    198\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_flat_features(x))\u001b[38;5;66;03m#usamos una función propia de la clase para obtener el número de características\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m#posteriormente tienen lugar las dos capas fully-connected\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    201\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m    202\u001b[0m \u001b[38;5;66;03m#y por último la capa densa que va a proporcionarnos la predicción\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_py39\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4x1024 and 512x512)"
     ]
    }
   ],
   "source": [
    "modelo = crea_Rajagopalan(5,1.0,'512/256')\n",
    "#definimos como loss la función de tipo cross entropy \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "#en este caso el optimizador será la función Adam (ampliamente utilizada)\n",
    "optimizer = torch.optim.Adam(params = modelo.parameters()) #dejamos el valor de learning-rate por defecto (0.001)\n",
    "acc,loss = entrena(modelo,epocas,train_loader,optimizer,criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
