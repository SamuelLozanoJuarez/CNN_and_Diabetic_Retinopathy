{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65292e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samue\\anaconda3\\envs\\venv_py39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#primero importamos todos los paquetes necesarios\n",
    "import torch #contiene todas las funciones de PyTorch\n",
    "import torch.nn as nn #contiene la clase padre de todos los modelos (nn.Module)\n",
    "import torch.nn.functional as F #esencial para la función de activación \n",
    "import torchvision #fundamental para la importación de imágenes\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt #para poder representar las gráficas\n",
    "import numpy as np #para las métricas de la red\n",
    "\n",
    "#importamos también las funcioness definidas para el entrenamiento y puesta a prueba de los modelos\n",
    "from modules.CNN_utilities import entrena, representa_test, representa_train, tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9884addd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de datos de train: 113\n",
      "Tamaño del conjunto de datos de test de Samsung: 93\n",
      "Tamaño del conjunto de datos de test de iPhone: 99\n"
     ]
    }
   ],
   "source": [
    "#establecemos el tamaño del batch, la escala de las imágenes y el número de épocas de entrenamiento\n",
    "batch = 4\n",
    "#en la arquitectura propuesta por Mobeen no se especifica ninguna escala, por lo que se empleará una escala cualquiera (512 por ejemplo)\n",
    "escala = 512\n",
    "epocas = 50\n",
    "\n",
    "#a continuación definimos la operación que permitirá transformar las imágenes del repositorio en Tensores que puedan ser empleados por PyTorch\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), #transforma la imagen de formato PIL a formato tensor\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), #normaliza el tensor para que la media de sus valores sea 0 y su desviación estándar 0.5\n",
    "     transforms.Resize((escala, escala))]) #redimensionamos las imágenes\n",
    "\n",
    "#a continuación cargamos el conjunto de imágenes de train (OCT) y los dos de test (iPhone y Samsung)\n",
    "OCT = ImageFolder(root = 'Datos/Classified Data/Images/OCT', transform = transform)\n",
    "print(f'Tamaño del conjunto de datos de train: {len(OCT)}')\n",
    "\n",
    "Samsung = ImageFolder(root = 'Datos/Classified Data/Images/Samsung', transform = transform)\n",
    "print(f'Tamaño del conjunto de datos de test de Samsung: {len(Samsung)}')\n",
    "\n",
    "iPhone = ImageFolder(root = 'Datos/Classified Data/Images/iPhone', transform = transform)\n",
    "print(f'Tamaño del conjunto de datos de test de iPhone: {len(iPhone)}')\n",
    "\n",
    "#establecemos una lista con el nombre de las etiquetas\n",
    "classes = OCT.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eaf952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y definimos también las funciones que van a ir cargando las imágenes en el modelo\n",
    "train_loader = DataLoader(\n",
    "    dataset = OCT,\n",
    "    batch_size = 4, #establecemos un tamaño de lote (batch_size) de 4, ya que son pocas imágenes y podemos permitírnoslo\n",
    "    shuffle = True, #indicamos que mezcle las imágenes\n",
    "    num_workers = 2 #genera subprocesos para cargar los datos y así liberamos el proceso main\n",
    ")\n",
    "\n",
    "test_S_loader = DataLoader(\n",
    "    dataset = Samsung,\n",
    "    batch_size = 4, #establecemos un tamaño de lote (batch_size) de 10, ya que son pocas imágenes y podemos permitírnoslo\n",
    "    shuffle = True, #indicamos que mezcle las imágenes\n",
    "    num_workers = 2 #genera subprocesos para cargar los datos y así liberamos el proceso main\n",
    ")\n",
    "\n",
    "test_i_loader = DataLoader(\n",
    "    dataset = iPhone,\n",
    "    batch_size = 4, #establecemos un tamaño de lote (batch_size) de 10, ya que son pocas imágenes y podemos permitírnoslo\n",
    "    shuffle = True, #indicamos que mezcle las imágenes\n",
    "    num_workers = 2 #genera subprocesos para cargar los datos y así liberamos el proceso main\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24965bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A lo largo de este script voy a probar a variar algunos parámetros del modelo (intentando no perder la esencia de la estructura original)\n",
    "#Los parámetros modificados serán los siguientes:\n",
    "# - inclusión o no de una capa convolucional adicional con un kernel = 3 y número de filtros = 16 o 32\n",
    "# - modificar el número de neuronas de las capas fully-connected:\n",
    "#        * probar con las combinaciones 256/128, 128/64, 64/32 y original (100/50)\n",
    "#\n",
    "#por tanto existen 12 posibles variaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68a7bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "#Primera Variante: sin capa convolucional adicional y combinación fully-connected 256/128\n",
    "class Primera_var(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        #sobreescribimos el constructor del padre\n",
    "        super(Primera_var,self).__init__()\n",
    "        #creamos la primera capa convolucional\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels = 3, #3 canales de entrada porque las imágenes son a color\n",
    "            out_channels = 4, #se trata del número de salidas de la capa. Es el número de kernels de la capa convolucional\n",
    "            kernel_size = 3, #suele tratarse de un número impar\n",
    "            stride = 1, #cantidad píxeles que se desplaza el filtro sobre la imagen\n",
    "            padding = 2, #cantidad de relleno que se va a aplicar sobre los bordes de la imagen\n",
    "        )\n",
    "        \n",
    "        #creamos la segunda capa convolucional\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels = 4, #3 canales de entrada porque las imágenes son a color\n",
    "            out_channels = 16, #se trata del número de salidas de la capa. Es el número de kernels de la capa convolucional\n",
    "            kernel_size = 3, #suele tratarse de un número impar\n",
    "            stride = 1, #cantidad píxeles que se desplaza el filtro sobre la imagen\n",
    "            padding = 2, #cantidad de relleno que se va a aplicar sobre los bordes de la imagen\n",
    "        )\n",
    "        \n",
    "        #definimos la capa de maxpooling\n",
    "        self.pool = nn.MaxPool2d(\n",
    "            kernel_size = 2, #establecemos el tamaño del kernel a 2*2\n",
    "            stride = 2 #cantidad píxeles que se desplaza el filtro sobre la imagen (por defecto se desplazará el tamaño del kernel)\n",
    "        )\n",
    "        \n",
    "        #y continuamos con las capas de neuronas fully-connected\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features = 16*129*129, #número de parámetros de entrada de la red\n",
    "            out_features = 256\n",
    "        )\n",
    "        \n",
    "        #segunda capa fully-connected\n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        \n",
    "        #y la última capa de fully connected que va a ser la que proporcione la última predicción\n",
    "        self.fc3 = nn.Linear(128,5)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #en esta función es donde tiene lugar la computación (y la función invocada por defecto al ejecutar la red)\n",
    "        #en esta ocasión la función de activación que emplearemos es ReLU\n",
    "        #primero tiene lugar la primera capa convolucional, con su respectiva activación y pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        #posteriormente la segunda capa convolucional, con ReLU y pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        #posteriormente la capa de flatten que convierte los datos de forma matricial a vectorial, para poder trabajar en las capas neuronales\n",
    "        x = x.view(-1,self.num_flat_features(x))#usamos una función propia de la clase para obtener el número de características\n",
    "        #por último las 3 capas neuronales fully-connected, con su correspondiente activación\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self,x):\n",
    "        #por último definimos la función que permite obtener el número de características de los tensores\n",
    "        size = x.size()[1:] #seleccionamos todas las dimensiones expcepto la primera (que son los batches)\n",
    "        num_features = 1\n",
    "        #va iterando y calcula el número de características de los datos (x)\n",
    "        for s in size:\n",
    "            num_features*=s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4044c3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primera_var(\n",
      "  (conv1): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=266256, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#una vez creada la estructura generamos una instancia de la misma y mostramos sus capas\n",
    "primera = Primera_var()\n",
    "print(primera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9200f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#definimos como loss la función de tipo cross entropy \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "#en este caso el optimizador será la función Adam (ampliamente utilizada)\n",
    "optimizer = torch.optim.Adam(params = primera.parameters()) #dejamos el valor de learning rate por defecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184bd97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entrenamos la red haciendo uso de la función 'entrena()' importada\n",
    "#recogemos los resultados en 2 variables que posteriormente nos permitirán representar la evolución de accuracy y loss\n",
    "acc,loss = entrena(primera,epocas,train_loader,optimizer,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f388db31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#representamos la evolución temporal de accuracy\n",
    "representa_train(acc,'Accuracy','Mobeen - Primera Variante')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fd5bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y representamos el loss\n",
    "representa_train(acc,'Loss','Mobeen - Primera Variante')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec09c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finalmente ponemos a prueba la red usando la función tester y recogemos los resultados para obtener las métricas\n",
    "y_true_iphone, y_pred_iphone, predictions_iphone = tester(primera,test_i_loader)\n",
    "#y posteriormente obtenemos y mostramos las métricas\n",
    "representa_test(y_true_iphone,y_pred_iphone,predictions_iphone,'iPhone','Primera Variante')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d14446",
   "metadata": {},
   "outputs": [],
   "source": [
    "#repetimos el mismo proceso pero empleando el conjunto de imágenes de Samsung\n",
    "y_true_samsung, y_pred_samsung, predictions_samsung = tester(primera,test_S_loader)\n",
    "#y posteriormente obtenemos y mostramos las métricas\n",
    "representa_test(y_true_samsung,y_pred_samsung,predictions_samsung,'Samsung','Primera Variante')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
