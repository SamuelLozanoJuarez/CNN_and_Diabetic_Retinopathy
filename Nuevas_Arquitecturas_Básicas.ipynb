{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d99b7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samue\\anaconda3\\envs\\venv_py39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#primero importamos todos los paquetes necesarios\n",
    "import torch #contiene todas las funciones de PyTorch\n",
    "import torch.nn as nn #contiene la clase padre de todos los modelos (nn.Module)\n",
    "import torch.nn.functional as F #esencial para la función de activación \n",
    "import torchvision #fundamental para la importación de imágenes\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c6bc122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importamos los paquetes necesarios para el cálculo de las métricas\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c85266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#en este script se entrenarán modelos que siguen una estructura idéntica a la relatada en los siguientes artículos:\n",
    "# - Automatic Detection and Classification of Diabetic Retinopathy stages using CNN (Ghosh R.,Ghosh K.)\n",
    "# - Classification of Diabetic Retinopathy Images Based on Customised CNN Architecture (Mobeen-ur-Rehman)\n",
    "# - Diagnosis of retinal disorders from Optical Coherence Tomography images using CNN (Rajagopalan N., Venkateswaran N.)\n",
    "# - AOCT-NET: a convolutional network automated classification of multiclass retinal diseases using spectral-domain optical coherence tomography images (Alqudah A.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85007421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#establecemos el tamaño del batch, la escala de las imágenes y el número de épocas de entrenamiento\n",
    "#debido a que cada una de las arquitecturas requiere una escala de imagen específica, la escala y los loaders serán definidas para cada arquitectura\n",
    "batch = 4\n",
    "#comenzaremos con la arquitectura propuesta por Ghosh, con una escala de 512, 512, 3\n",
    "escala = 512\n",
    "epocas = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "583ccb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a continuación definimos la operación que permitirá transformar las imágenes del repositorio en Tensores que puedan ser empleados por PyTorch\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), #transforma la imagen de formato PIL a formato tensor\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), #normaliza el tensor para que la media de sus valores sea 0 y su desviación estándar 0.5\n",
    "     transforms.Resize((escala, escala))]) #redimensionamos las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f2ffcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de datos de train: 113\n",
      "Tamaño del conjunto de datos de test de Samsung: 93\n",
      "Tamaño del conjunto de datos de test de iPhone: 99\n"
     ]
    }
   ],
   "source": [
    "#a continuación cargamos el conjunto de imágenes de train (OCT) y los dos de test (iPhone y Samsung)\n",
    "OCT = ImageFolder(root = 'Datos/Classified Data/Images/OCT', transform = transform)\n",
    "print(f'Tamaño del conjunto de datos de train: {len(OCT)}')\n",
    "\n",
    "Samsung = ImageFolder(root = 'Datos/Classified Data/Images/Samsung', transform = transform)\n",
    "print(f'Tamaño del conjunto de datos de test de Samsung: {len(Samsung)}')\n",
    "\n",
    "iPhone = ImageFolder(root = 'Datos/Classified Data/Images/iPhone', transform = transform)\n",
    "print(f'Tamaño del conjunto de datos de test de iPhone: {len(iPhone)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d587ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#establecemos una lista con el nombre de las etiquetas\n",
    "classes = OCT.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3039dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y definimos también las funciones que van a ir cargando las imágenes en el modelo\n",
    "train_loader = DataLoader(\n",
    "    dataset = OCT,\n",
    "    batch_size = 4, #establecemos un tamaño de lote (batch_size) de 4, ya que son pocas imágenes y podemos permitírnoslo\n",
    "    shuffle = True, #indicamos que mezcle las imágenes\n",
    "    num_workers = 2 #genera subprocesos para cargar los datos y así liberamos el proceso main\n",
    ")\n",
    "\n",
    "test_S_loader = DataLoader(\n",
    "    dataset = Samsung,\n",
    "    batch_size = 4, #establecemos un tamaño de lote (batch_size) de 10, ya que son pocas imágenes y podemos permitírnoslo\n",
    "    shuffle = True, #indicamos que mezcle las imágenes\n",
    "    num_workers = 2 #genera subprocesos para cargar los datos y así liberamos el proceso main\n",
    ")\n",
    "\n",
    "test_i_loader = DataLoader(\n",
    "    dataset = iPhone,\n",
    "    batch_size = 4, #establecemos un tamaño de lote (batch_size) de 10, ya que son pocas imágenes y podemos permitírnoslo\n",
    "    shuffle = True, #indicamos que mezcle las imágenes\n",
    "    num_workers = 2 #genera subprocesos para cargar los datos y así liberamos el proceso main\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5840ddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#en esta ocasión, para mayor sencillez del código, vamos a omitir el paso de representación de las imágenes (además sabemos que los DataLoaders funcionan correctamente pues son los mismos que los empleados en el script 'Primera_Red_Básica')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5baae0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARQUITECTURA DEFINIDA POR GHOSH\n",
    "class Ghosh(nn.Module):\n",
    "    #esta estructura está formada por capas convolucionales, de maxpooling, de activación, de Dropout, fully-connected y de clasificación\n",
    "    \n",
    "    def __init__(self):\n",
    "        #sobreescribimos el constructor del padre\n",
    "        super(Ghosh,self).__init__()\n",
    "        #primero definimos una capa convolucional\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels = 3, #3 canales de entrada porque las imágenes son a color\n",
    "            out_channels = 6, #se trata del número de salidas de la capa. Puede tratarse de un valor arbitrario\n",
    "            kernel_size = 7, #suele tratarse de un número impar\n",
    "            stride = 2, #cantidad píxeles que se desplaza el filtro sobre la imagen\n",
    "            padding = 2, #cantidad de relleno que se va a aplicar sobre los bordes de la imagen\n",
    "        )\n",
    "        #una segunda capa convolucional\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels = 6, #6 canales de entrada porque es el número de salidas de la capa anterior\n",
    "            out_channels = 6, #en este caso deben coincidir entradas y salidas para que al llamar consecutivamente a dos capas convoucionales no haya problemas\n",
    "            kernel_size = 3, #suele tratarse de un número impar\n",
    "            stride = 1, #cantidad píxeles que se desplaza el filtro sobre la imagen\n",
    "            padding = 1, #cantidad de relleno que se va a aplicar sobre los bordes de la imagen\n",
    "        )\n",
    "        #la función de activación (en este caso PReLU)\n",
    "        self.activation = nn.PReLU()\n",
    "        #la capa de MaxPool\n",
    "        self.pool = nn.MaxPool2d(\n",
    "            kernel_size = 2, #establecemos el tamaño del kernel a 2*2\n",
    "            stride = 2 #cantidad píxeles que se desplaza el filtro sobre la imagen\n",
    "        )\n",
    "        #la primera capa de neuronas a la que aplicaremos Dropout como técnica de regularización\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features = 294, #número de parámetros de entrada de la red (los valores se obtienen experimentalmente)\n",
    "            out_features = 256 #número de neuronas de salida\n",
    "        )\n",
    "        \n",
    "        #la segunda capa fully-connected\n",
    "        self.fc2 = nn.Linear(256,1024)\n",
    "        \n",
    "        #la tercera capa fully-connected\n",
    "        self.fc3 = nn.Linear(1024,512)\n",
    "        \n",
    "        #la capa de neuronas fully-connected\n",
    "        self.dense = nn.Linear(\n",
    "            in_features = 512, #número de parámetros de entrada de la red (los valores se obtienen experimentalmente)\n",
    "            out_features = 5 #número de neuronas de salida\n",
    "        )\n",
    "        #por último la capa de Softmax, que convierte los valores del Tensor predicho en probabilidades\n",
    "        self.softmax = nn.Softmax(\n",
    "            dim = 1 #dimensión sobre la que debe actuar softmax \n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #en esta función es donde tiene lugar la computación (y la función invocada por defecto al ejecutar la red)\n",
    "        x = self.pool(self.activation(self.conv1(x)))\n",
    "        x = self.activation(self.conv2(x))\n",
    "        x = self.pool(self.activation(self.conv2(x)))\n",
    "        x = self.activation(self.conv2(x))\n",
    "        x = self.pool(self.activation(self.conv2(x)))\n",
    "        x = self.activation(self.conv2(x))\n",
    "        x = self.activation(self.conv2(x))\n",
    "        x = self.activation(self.conv2(x))\n",
    "        x = self.pool(self.activation(self.conv2(x)))\n",
    "        x = self.activation(self.conv2(x))\n",
    "        x = self.activation(self.conv2(x))\n",
    "        x = self.activation(self.conv2(x))\n",
    "        x = self.pool(self.activation(self.conv2(x)))\n",
    "        x = x.view(-1,self.num_flat_features(x)) #usamos una función propia de la clase para obtener el número de características\n",
    "        x = F.dropout(self.fc1(x))\n",
    "        x = F.dropout(self.fc2(x))\n",
    "        x = F.dropout(self.fc3(x))\n",
    "        x = self.dense(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self,x):\n",
    "        #por último definimos la función que permite obtener el número de características de los tensores\n",
    "        size = x.size()[1:] #seleccionamos todas las dimensiones expcepto la primera (que son los batches)\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features*=s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7d8afde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ghosh(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(7, 7), stride=(2, 2), padding=(2, 2))\n",
      "  (conv2): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activation): PReLU(num_parameters=1)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=294, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=1024, bias=True)\n",
      "  (fc3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=5, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ghosh = Ghosh()\n",
    "print(ghosh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f91557fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#definimos como loss la función de tipo cross entropy \n",
    "criterion = nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4ddbd5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#en este caso el optimizador será la función Adam (ampliamente utilizada)\n",
    "optimizer = torch.optim.Adam(params = ghosh.parameters()) #dejamos el valor de learning rate por defecto (0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c107620c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1/50 - Accuracy: 0.5132743362831859 - Loss: 1.9048324823379517\n",
      "Época 2/50 - Accuracy: 0.5132743362831859 - Loss: 0.9048324823379517\n",
      "Época 3/50 - Accuracy: 0.5132743362831859 - Loss: 1.9048324823379517\n",
      "Época 4/50 - Accuracy: 0.5132743362831859 - Loss: 1.9048324823379517\n",
      "Época 5/50 - Accuracy: 0.5132743362831859 - Loss: 0.9048324823379517\n",
      "Época 6/50 - Accuracy: 0.5132743362831859 - Loss: 0.9048324823379517\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#propagamos hacia atrás el valor loss\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#y modificamos los pesos en función del loss y la función optimizer\u001b[39;00m\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_py39\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_py39\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#definimos 2 listas en las que almacenaremos los valores de accuracy y loss de cada época para poder graficarlo posteriormente\n",
    "acc_graph = []\n",
    "loss_graph = []\n",
    "#para entrenar el modelo vamos a iterar el número de épocas determinadas, calculando el valor de loss y accuracy para cada época\n",
    "for epoch in range(epocas):\n",
    "    #establecemos el número de predicciones correctas inicial a 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    #y cargamos las imágenes de entrenamiento y sus etiquetas usando la estructura Loader previamente creada\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        #establecemos a 0 los parámetros del modelo\n",
    "        optimizer.zero_grad()\n",
    "        #generamos las predicciones de los inputs\n",
    "        outputs = ghosh(inputs)\n",
    "        #calculamos el loss, la desviación de las predicciones con respecto a las etiquetas\n",
    "        loss = criterion(outputs, labels)\n",
    "        #propagamos hacia atrás el valor loss\n",
    "        loss.backward()\n",
    "        #y modificamos los pesos en función del loss y la función optimizer\n",
    "        optimizer.step()\n",
    "        #actualizamos el número de predicciones correctas\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    #una vez finalizada la época (que recorre todo el conjunto de imágenes) mostramos el valor del loss y del accuracy\n",
    "    print(f'Época {epoch +1}/{epocas} - Accuracy: {correct/len(OCT)} - Loss: {loss.data.item()}')\n",
    "    #añadimos los valores a la lista correspondiente\n",
    "    loss_graph.append(loss.data.item())\n",
    "    acc_graph.append(correct/len(OCT))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
